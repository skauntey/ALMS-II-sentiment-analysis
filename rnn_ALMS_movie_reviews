{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ALMS_Model1_250421",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skauntey/ALMS-II-sentiment-analysis/blob/main/rnn_ALMS_movie_reviews\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMMBcG70TU7Z"
      },
      "source": [
        "# You may have to download the modules below\n",
        "# Especially if you're using Colab\n",
        "\n",
        "! pip install -q kaggle\n",
        "! pip install gitpython\n",
        "! pip install wget"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wq34HOvwljNQ"
      },
      "source": [
        "### Installing Kaggle API in local computer / or on Colab\n",
        "##### Make sure that kaggle.json file is available in home/.kaggle folder\n",
        "##### Follow the 'API Credentials' on the Kaggle page (below) to find how to install Kaggle API.\n",
        "##### https://github.com/Kaggle/kaggle-api"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD8yGUsRTYDY"
      },
      "source": [
        "# Run this cell if you're using Google Colab\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Y53r-IDaX6kG"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, shutil\n",
        "import kaggle\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juC7D6JcZos3"
      },
      "source": [
        "os.listdir('/content')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlNLU8XGlqFI"
      },
      "source": [
        "# Clone github package to work on local computer\n",
        "# Change 'content' folder to your destination\n",
        "# Make sure you remove '#' before cloning\n",
        "\n",
        "import git\n",
        "import os\n",
        "repo = \"https://github.com/skauntey/ALMS-II-sentiment-analysis.git\"\n",
        "file_path = str(os.getcwd()) # This is a folder location where to save files\n",
        "def repo_download(repo):\n",
        "    current_folder = str(os.getcwd())\n",
        "    if current_folder.split('/')[-1] != 'ALMS-II-sentiment-analysis':\n",
        "        if os.path.isdir('ALMS-II-sentiment-analysis'):\n",
        "            print ('Repo exists!')\n",
        "        else:\n",
        "          git.Git(file_path).clone(repo)\n",
        "    \n",
        "    print('Repo cloned!)')\n",
        "\n",
        "repo_download(repo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fzv2emWya-dS"
      },
      "source": [
        "current_folder = str(os.getcwd())\n",
        "if current_folder.split('/')[-1] != 'ALMS-II-sentiment-analysis':\n",
        "  if os.path.isdir('ALMS-II-sentiment-analysis'):\n",
        "    os.chdir(os.path.join(current_folder, 'ALMS-II-sentiment-analysis'))\n",
        "else:\n",
        "  pass\n",
        "print(os.getcwd()) #make sure you are in ALMS-II-sentiment-analysis folder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "YJfWNzEsTObM"
      },
      "source": [
        "## 2. Downloading Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "iaLMnaPBTObN"
      },
      "source": [
        "## In order to download the dataset make sure that kaggle.json file is available in ~/.kaggle/kaggle.json\n",
        "# 'Kaggle.json' Downloading instructions are available under 'API Credential' heading of the github page\n",
        "# https://github.com/Kaggle/kaggle-api\n",
        "\n",
        "import zipfile\n",
        "#Define file to download\n",
        "\n",
        "dataset = \"sentiment-analysis-on-movie-reviews\"\n",
        "\n",
        "def kaggle_dataset_download(dataset):\n",
        "    kaggle.api.authenticate()\n",
        "    kaggle.api.competition_download_files(dataset, path= str(os.getcwd())+\"/dataset/\", force = True)\n",
        "    kaggle.api.dataset_download_files('danielwillgeorge/glove6b100dtxt', path= str(os.getcwd())+\"/glove_data/\", force = True, unzip=True)\n",
        "    print ('Data downloaded!')\n",
        "\n",
        "def unzip_kaggle_file():\n",
        "\n",
        "    # file_path\n",
        "    zipped_file = os.listdir('dataset')[0]\n",
        "    file_path = os.path.join(str(os.getcwd()), 'dataset', zipped_file)\n",
        "    folder_path = os.path.join(str(os.getcwd()), 'dataset')\n",
        "\n",
        "    # making sure that previously downloaded file, if any, is deleted before\n",
        "    for file in os.listdir(folder_path):\n",
        "        path = os.path.join(os.getcwd(), 'dataset', file)\n",
        "        if path.split('.')[-1] != \"zip\":\n",
        "            try:\n",
        "                shutil.rmtree(path)\n",
        "            except:\n",
        "                raise\n",
        "\n",
        "    # Unzipping the file\n",
        "    with zipfile.ZipFile(file_path) as zip_file:\n",
        "        for member in zip_file.namelist():\n",
        "            if member.split('.')[-1] == \"zip\":\n",
        "                fdir = member.split('.')[0]\n",
        "                zip_file.extract(member, path= os.path.join('dataset/'+fdir))\n",
        "                # extracting individual train and test files in their respective folders\n",
        "                zippedfile_name = os.listdir(os.path.join(str(os.getcwd()+'/dataset/'+fdir+'/')))\n",
        "                zippedfile = os.path.join(str(os.getcwd())+'/dataset/'+ fdir, zippedfile_name[0])\n",
        "                with zipfile.ZipFile(zippedfile, mode='r') as tsv_zip:\n",
        "                    tsv_zip.extractall(path = os.path.join('dataset/'+fdir+'/'))\n",
        "\n",
        "                os.remove(zippedfile)\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "    os.remove(os.path.join(str(os.getcwd()), 'dataset', zipped_file))\n",
        "    print('Kaggle file is downloaded and unzipped!')\n",
        "\n",
        "\n",
        "kaggle_dataset_download(dataset)\n",
        "unzip_kaggle_file()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XR-Gq7oRoDCg",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## 3. Converting files to DATASETs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD4Ek0KzTpXp"
      },
      "source": [
        "#C:\\Users\\eq\\Documents\\UCL\\ELEC0135 ALMS II\\ALMS-II-sentiment-analysis\\dataset\\train\n",
        "file_path = os.path.join(str(os.getcwd()), 'dataset','train')\n",
        "file = os.listdir(file_path)[0]\n",
        "path = os.path.join(file_path, file)\n",
        "# read the data\n",
        "train_data = pd.read_csv(path, sep=\"\\t\")\n",
        "label_class = [('negative', 0), ('somewhat negative', 1), ('neutral', 2), ('somewhat positive', 3), ('positive', 4)]\n",
        "\n",
        "pdf = pd.DataFrame({\n",
        "    'sentence_id' : train_data['SentenceId'],\n",
        "    'text_reviews': train_data['Phrase'],\n",
        "    'class_id': train_data['Sentiment'],\n",
        "    'sentence_len': [len(x) for x in train_data['Phrase']],\n",
        "    })\n",
        "\n",
        "pdf['class_'] = [label_class[i][0] for i in pdf['class_id']]\n",
        "\n",
        "pdf = pdf[pdf['sentence_len'] > 0]\n",
        "\n",
        "pdf_data = pdf['text_reviews']\n",
        "pdf_labels = pdf['class_id']\n",
        "\n",
        "print(pdf_data.shape)\n",
        "print(pdf_labels.shape)\n",
        "pdf.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "1OpavJNaTObR"
      },
      "source": [
        "## 3.1 Pre-processing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "1YLbT-OHTObS"
      },
      "source": [
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
        "\n",
        "def clean_text_func(text):\n",
        "    text = text.lower() # lowercase text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
        "    return text\n",
        "\n",
        "def preprocessing_dataset(pdf_data, pdf):\n",
        "    pdf_data = pdf_data.apply(clean_text_func)\n",
        "    text_dataset = pdf[pdf_data.astype(bool)] \n",
        "    pdf = text_dataset.reset_index(drop = True)\n",
        "\n",
        "    text_data = pdf['text_reviews']\n",
        "    pdf_text_data = text_data.apply(clean_text_func)\n",
        "    pdf['text_reviews'] = pdf_text_data\n",
        "    return pdf\n",
        "\n",
        "pdf = preprocessing_dataset(pdf_data,pdf)\n",
        "pdf.head()\n",
        "pdf.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRaIhOQRUA5o"
      },
      "source": [
        "## 3.1.1 Creating a smaller dataset (DATASET B)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6rNN_WgkrDp"
      },
      "source": [
        "sen_numbers = [x for x in set(pdf['sentence_id'])]\n",
        "def de_agument_text(pdf):\n",
        "  strings  = []\n",
        "  for i in sen_numbers:\n",
        "    anew = pdf.loc[pdf['sentence_id'] == i].sort_values('sentence_len', ascending=False)\n",
        "    seriess = anew[['text_reviews', 'class_id', 'sentence_len', 'class_']].iloc[0,0:4].values\n",
        "    strings.append(seriess)\n",
        "  return strings\n",
        "\n",
        "pdf_compressed = pd.DataFrame(de_agument_text(pdf), columns=['text_reviews', 'class_id', 'sentence_len', 'class_'])\n",
        "print(pdf_compressed.head())\n",
        "print('Size of DATASET-A {}'.format(pdf.shape))\n",
        "print('Size of DATASET-B {}'.format(pdf_compressed.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dW3RQ_QU3jo"
      },
      "source": [
        "## 3.2 Data Representation (DATASET A & B)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC5PXA2eVYTG"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "labels = {'negative':0,'somewhat negative':1,'neutral': 2,'somewhat positive':3,'positive':4}\n",
        "x = labels.keys()\n",
        "\n",
        "y_compressed = pdf_compressed.groupby('class_id').text_reviews.count()\n",
        "\n",
        "y_pdf = pdf.groupby('class_id').text_reviews.count()\n",
        "\n",
        "\n",
        "fig, [ax1, ax2]= plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
        "\n",
        "ax2.bar(x, y_compressed)\n",
        "ax2.set_xticklabels(labels = x, rotation =45)\n",
        "ax2.set_xlabel('DATASET-B: Filtered based on Sentence Id', fontsize = 15)\n",
        "ax2.set_ylabel('Number of reviews in each class', fontsize = 15)\n",
        "\n",
        "ax1.bar(x, y_pdf)\n",
        "ax1.set_xticklabels(labels = x, rotation =45)\n",
        "ax1.set_xlabel('DATASET-A: Original Kaggle dataset', fontsize = 15)\n",
        "ax1.set_ylabel('Number of reviews in each class', fontsize = 15)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lv6c9XDXjoL"
      },
      "source": [
        "### 3.2.1 Labels representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fXnRcP_Xkej"
      },
      "source": [
        "from io import StringIO\n",
        "id_class_df = pdf[['class_id', 'class_']].drop_duplicates().sort_values('class_id')\n",
        "id_class = dict(id_class_df.values)\n",
        "class_id_dict = dict(id_class_df[['class_', 'class_id']].values)\n",
        "class_id_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "ZkxV-o0KTObU"
      },
      "source": [
        "## 4. train_test_split of datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9UyoFBLX0OG"
      },
      "source": [
        "# train_test split of DATASET A and DATASET B\n",
        "\n",
        "\n",
        "print (\"Numpy array of Original Dataset\")\n",
        "X_corpus_A = pdf.text_reviews.to_numpy()\n",
        "y_labels_A = pdf.class_id.to_numpy()\n",
        "X_corpus_B = pdf_compressed.text_reviews.to_numpy()\n",
        "y_labels_B = pdf_compressed.class_id.to_numpy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ob1UCgzYay3"
      },
      "source": [
        "#Verifying the datas shapes and type\n",
        "print ('-' * 50)\n",
        "print (\"Info of Original Dataset\")\n",
        "print(type(X_corpus_A))\n",
        "print(type(y_labels_A))\n",
        "print(X_corpus_A.shape)\n",
        "print(y_labels_A.shape)\n",
        "\n",
        "print(type(X_corpus_B))\n",
        "print(type(y_labels_B))\n",
        "print(X_corpus_B.shape)\n",
        "print(y_labels_B.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "jgtKcRdeTObU"
      },
      "source": [
        "## 5. Tokenzing and Padding of Text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "Cvg5DrG6TObV"
      },
      "source": [
        "#### 5.1 Tokenizing and Padding using tensorflow Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zB5IeT9FVIT"
      },
      "source": [
        "# Downloading TensorFlow modules\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwxpvieC45wJ"
      },
      "source": [
        "# calculating a total number of words from a clean dataset\n",
        "def num_words (text_dataset):\n",
        "    total_words = {}\n",
        "    for x in text_dataset:\n",
        "        for a in x.split():\n",
        "            if a not in total_words.keys():\n",
        "                total_words[a] = 1\n",
        "            else:\n",
        "                total_words[a] += 1\n",
        "    return len(total_words.keys())\n",
        "\n",
        "num_words_A = num_words(X_corpus_A)\n",
        "print ('word count in DATASET- A {}.'.format(num_words_A))\n",
        "num_words_B = num_words(X_corpus_B)\n",
        "print ('word count in DATASET- B {}.'.format(num_words_B))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI9ejNlo3Fr7"
      },
      "source": [
        "def tokenizing_dataset(text):\n",
        "    tokenizer = Tokenizer(oov_token='<00V>')\n",
        "    tokenizer.fit_on_texts(text)\n",
        "    word_index = tokenizer.word_index\n",
        "    return word_index, tokenizer\n",
        "\n",
        "def max_len(text):\n",
        "    _, tokenizer = tokenizing_dataset(text)\n",
        "    sequences = tokenizer.texts_to_sequences(text)\n",
        "    max_len = max([len(x) for x in sequences])\n",
        "    return max_len, sequences\n",
        "\n",
        "max_len_A,sequences_A = max_len(X_corpus_A)\n",
        "max_len_B,sequences_B = max_len(X_corpus_B)\n",
        "\n",
        "def sequencing_padding_dataset(text, max_len):\n",
        "    word_index, tokenizer = tokenizing_dataset(text)\n",
        "    text_sequences = tokenizer.texts_to_sequences(text)\n",
        "    padded_text = pad_sequences(text_sequences, maxlen= max_len, truncating ='post', padding='post')\n",
        "    return padded_text, text_sequences, word_index\n",
        "\n",
        "padded_X_A, X_sequences_A, X_word_index_A = sequencing_padding_dataset(X_corpus_A, max_len_A)\n",
        "padded_X_B, X_sequences_B, X_word_index_B = sequencing_padding_dataset(X_corpus_B, max_len_B)\n",
        "\n",
        "print('DATASET A info.')\n",
        "print(max_len_A)\n",
        "print(type(padded_X_A))\n",
        "print(type(X_sequences_A))\n",
        "print(padded_X_A.shape)\n",
        "print(len(X_sequences_A))\n",
        "print(padded_X_A[0:2])\n",
        "print(X_sequences_A[0:2])\n",
        "\n",
        "print('DATASET B info.')\n",
        "print(max_len_B)\n",
        "print(type(padded_X_B))\n",
        "print(type(X_sequences_B))\n",
        "print(padded_X_B.shape)\n",
        "print(len(X_sequences_B))\n",
        "print(padded_X_B[0:2])\n",
        "print(X_sequences_B[0:2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "3bkpaVk-TObW"
      },
      "source": [
        "#### 5.1.2 Train and Test split of the Padded Text and One-hot encoded Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "R35z8bXPTObW"
      },
      "source": [
        "# One-hot encoding of the label's dataset\n",
        "split_size = 0.2\n",
        "def one_hot_encoding (dataset_labels):\n",
        "    labels_onehot = pd.get_dummies(dataset_labels)\n",
        "    return labels_onehot\n",
        "labels_onehot_A = one_hot_encoding(y_labels_A)\n",
        "labels_onehot_B = one_hot_encoding(y_labels_B)\n",
        "\n",
        "# Train, Test and Split of Padded text and One-hot encoded labels\n",
        "X_train_A, X_test_A, Y_train_A_onehot, Y_test_A_onehot = train_test_split(padded_X_A, labels_onehot_A, test_size = split_size)\n",
        "X_train_B, X_test_B, Y_train_B_onehot, Y_test_B_onehot = train_test_split(padded_X_B, labels_onehot_B, test_size = split_size)\n",
        "print('Shape of dataset with one-hot encoding')\n",
        "print(X_train_A.shape,Y_train_A_onehot.shape)\n",
        "print(X_test_A.shape,Y_test_A_onehot.shape)\n",
        "print(X_train_B.shape,Y_train_B_onehot.shape)\n",
        "print(X_test_B.shape,Y_test_B_onehot.shape)\n",
        "\n",
        "# Train, Test and Split of Padded text for Sparse Categorical entropy\n",
        "X_train_A, X_test_A, Y_train_A, Y_test_A = train_test_split(padded_X_A, y_labels_A, test_size = split_size)\n",
        "X_train_B, X_test_B, Y_train_B, Y_test_B = train_test_split(padded_X_B, y_labels_B, test_size = split_size)\n",
        "print('Shape of dataset without one hot encoding')\n",
        "print(X_train_A.shape,Y_train_A.shape)\n",
        "print(X_test_A.shape,Y_test_A.shape)\n",
        "print(X_train_B.shape,Y_train_B.shape)\n",
        "print(X_test_B.shape,Y_test_B.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "_mSExeTlTObX"
      },
      "source": [
        "#### 5.1.3. Verifying padded sequence with the original text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sw8ipL5oDC0",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "def decoding_sequences(dataset_text, text_sequences):\n",
        "    word_index, _ = tokenizing_dataset(dataset_text)\n",
        "    reversed_index = dict(map(reversed, word_index.items()))\n",
        "    for letter in range(len(reversed_index)):\n",
        "        reversed_index.get(letter)\n",
        "\n",
        "    reversed_sequence = [reversed_index.get(letter) for letter in text_sequences]\n",
        "    return ' '.join(reversed_sequence)\n",
        "    \n",
        "# print(decoding_sequences(pdf_dataset_data, pdf_sequences))\n",
        "print(decoding_sequences(X_corpus_A, X_sequences_A[0]))\n",
        "print(X_corpus_A[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "5FqhxGbCTObX"
      },
      "source": [
        "### 5.2 Word Embedding using Pre-Trained Glove dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFS9rqcpfvzi",
        "scrolled": true
      },
      "source": [
        "# downloading module and packages\n",
        "#If error occurs please download the wget package in the cell above\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "import external_dataset\n",
        "from external_dataset.glove import download_GloveDataset\n",
        "from external_dataset.glove import embedding_GloveDataset\n",
        "\n",
        "url = 'danielwillgeorge/glove6b100dtxt'\n",
        "glove_path = download_GloveDataset(url)\n",
        "embedding_matrix, embedding_index = embedding_GloveDataset(glove_path, X_word_index_A)\n",
        "#print(embedding_matrix['the'])\n",
        "print(type(embedding_matrix))\n",
        "print(type(embedding_index))\n",
        "print(embedding_index['the'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "99Ni14PqTObY"
      },
      "source": [
        "## 6. Building Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "fPRwZe_cTObY"
      },
      "source": [
        "# downloading tensorflow modules\n",
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, LSTM, GRU, Dropout, Embedding, SpatialDropout1D, BatchNormalization, Bidirectional, Activation\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXSjtVkfG-ii"
      },
      "source": [
        "## remove/replace '#' to run different models*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J10R72x9-qEq"
      },
      "source": [
        "# importing modules from package\n",
        "from tf_models.rnn_lstm_gru import simple_rnn\n",
        "from tf_models.rnn_lstm_gru import uni_directional_LSTM\n",
        "from tf_models.rnn_lstm_gru import uni_directional_GRU\n",
        "from tf_models.rnn_lstm_gru import bi_directional_rnn_lstm\n",
        "\n",
        "\n",
        "from tf_models.rnn_lstm_gru import simple_rnn_glove\n",
        "from tf_models.rnn_lstm_gru import uni_directional_LSTM_glove\n",
        "from tf_models.rnn_lstm_gru import uni_directional_GRU_glove\n",
        "from tf_models.rnn_lstm_gru import bi_directional_rnn_lstm_glove\n",
        "\n",
        "vocabulary_size = len(X_word_index_A) + 1\n",
        "embedding_dim = 100\n",
        "\n",
        "max_length = X_train_A.shape[1]\n",
        "trunc_type = 'post'\n",
        "oov_tok = '<00V>'\n",
        "\n",
        "#model = simple_rnn(vocabulary_size, embedding_dim, max_length)\n",
        "#model = simple_rnn_glove(vocabulary_size, embedding_dim, max_length, embedding_matrix)\n",
        "\n",
        "#model = uni_directional_LSTM(vocabulary_size, embedding_dim, max_length)\n",
        "#model = uni_directional_LSTM_glove(vocabulary_size, embedding_dim, max_length, embedding_matrix)\n",
        "\n",
        "#model = uni_directional_GRU(vocabulary_size, embedding_dim, max_length)\n",
        "#model = uni_directional_GRU_glove(vocabulary_size, embedding_dim, max_length, embedding_matrix)\n",
        "\n",
        "#model = bi_directional_rnn_lstm(vocabulary_size, embedding_dim, max_length)\n",
        "model = bi_directional_rnn_lstm_glove(vocabulary_size, embedding_dim, max_length, embedding_matrix)\n",
        "\n",
        "model.compile(loss= tf.losses.SparseCategoricalCrossentropy(),\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-06),\n",
        "              metrics= ['accuracy'])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjTyKxhioDC3"
      },
      "source": [
        "num_epochs = 100\n",
        "batch_size = 512\n",
        "history = model.fit(X_train_A, Y_train_A, batch_size=batch_size, epochs = num_epochs, validation_data=(X_test_A, Y_test_A))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWUeqH3pjo5w"
      },
      "source": [
        "daccr = model.evaluate(X_test_A,Y_test_A)\n",
        "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXm7bhU0Kqyo"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "  \n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP-mDU-mRNao"
      },
      "source": [
        "e = model.layers[0]\n",
        "weights = e.get_weights()[0]\n",
        "print(weights.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDQgL14B8YLJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "OxUFUgrUX6kt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}