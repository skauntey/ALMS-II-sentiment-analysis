{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ALMS_Model1_250421",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skauntey/ALMS-II-sentiment-analysis/blob/main/5_ALMS_Model2_250421.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDjkrxKwJAGK"
      },
      "source": [
        "! pip install -q kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ztg9JalJMDn"
      },
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ix-DSBh2oDB0"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, shutil\n",
        "import kaggle\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "6eqm8yZfFkmL"
      },
      "source": [
        "## 2. Downloading kaggle competition files\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "VCvpCZSLFkmM"
      },
      "source": [
        "dataset = \"sentiment-analysis-on-movie-reviews\"\n",
        "def kaggle_dataset_download(dataset):\n",
        "    kaggle.api.authenticate()\n",
        "    kaggle.api.competition_download_files(dataset, path= str(os.getcwd())+\"/dataset/\", force = True)\n",
        "    print ('Data downloaded!')\n",
        "\n",
        "kaggle_dataset_download(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "3zqVG0QAFkmN"
      },
      "source": [
        "## In order to download the dataset make sure that kaggle.json file is available in ~/.kaggle/kaggle.json\n",
        "# 'Kaggle.json' Downloading instructions are available under 'API Credential' heading of the github page\n",
        "# https://github.com/Kaggle/kaggle-api\n",
        "\n",
        "import zipfile\n",
        "#Define file to download\n",
        "\n",
        "dataset = \"sentiment-analysis-on-movie-reviews\"\n",
        "\n",
        "def kaggle_dataset_download(dataset):\n",
        "    kaggle.api.authenticate()\n",
        "    kaggle.api.competition_download_files(dataset, path= str(os.getcwd())+\"/dataset/\", force = True)\n",
        "    print ('Data downloaded!')\n",
        "\n",
        "def unzip_kaggle_file():\n",
        "\n",
        "    # file_path\n",
        "    zipped_file = r'dataset/sentiment-analysis-on-movie-reviews.zip'\n",
        "    file_path = os.path.join(str(os.getcwd()),zipped_file)\n",
        "    folder_path = os.path.join(str(os.getcwd()), 'dataset')\n",
        "\n",
        "    # making sure that previously downloaded file, if any, is deleted before\n",
        "    for file in os.listdir(folder_path):\n",
        "        path = os.path.join(os.getcwd(), 'dataset', file)\n",
        "        if not path.split('.')[-1] == \"zip\":\n",
        "          shutil.rmtree(path)\n",
        "        else:\n",
        "          continue\n",
        "\n",
        "    # Unzipping the file\n",
        "    with zipfile.ZipFile(file_path) as zip_file:\n",
        "        for member in zip_file.namelist():\n",
        "            if member.split('.')[-1] == \"zip\":\n",
        "                fdir = member.split('.')[0]\n",
        "                zip_file.extract(member, path= os.path.join('dataset/'+fdir))\n",
        "                # extracting individual train and test files in their respective folders\n",
        "                zippedfile_name = os.listdir(os.path.join(str(os.getcwd()+'/dataset/'+fdir+'/')))\n",
        "                zippedfile = os.path.join(str(os.getcwd())+'/dataset/'+ fdir, zippedfile_name[0])\n",
        "                with zipfile.ZipFile(zippedfile, mode='r') as tsv_zip:\n",
        "                    tsv_zip.extractall(path = os.path.join('dataset/'+fdir+'/'))\n",
        "\n",
        "                os.remove(zippedfile)\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "    os.remove(zipped_file)\n",
        "    print('Kaggle file is downloaded and unzipped!')\n",
        "\n",
        "\n",
        "kaggle_dataset_download(dataset)\n",
        "unzip_kaggle_file()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XR-Gq7oRoDCg",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# 3. Converting files to DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYK76UoboDCh",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "#C:\\Users\\eq\\Documents\\UCL\\ELEC0135 ALMS II\\ALMS-II-sentiment-analysis\\dataset\\train\n",
        "file_path = os.path.join(str(os.getcwd()), 'dataset','train')\n",
        "file = os.listdir(file_path)[0]\n",
        "path = os.path.join(file_path, file)\n",
        "# read the data\n",
        "train_data = pd.read_csv(path, sep=\"\\t\")\n",
        "label_class = [('negative', 0), ('somewhat negative', 1), ('neutral', 2), ('somewhat positive', 3), ('positive', 4)]\n",
        "\n",
        "pdf = pd.DataFrame({\n",
        "    'sentence_id' : train_data['SentenceId'],\n",
        "    'text_reviews': train_data['Phrase'],\n",
        "    'class_id': train_data['Sentiment'],\n",
        "    'sentence_len': [len(x) for x in train_data['Phrase']],\n",
        "    })\n",
        "\n",
        "pdf['class_'] = [label_class[i][0] for i in pdf['class_id']]\n",
        "\n",
        "pdf = pdf[pdf['sentence_len'] > 15]\n",
        "\n",
        "pdf_data = pdf['text_reviews']\n",
        "pdf_labels = pdf['class_id']\n",
        "\n",
        "print(pdf_data.shape)\n",
        "print(pdf_labels.shape)\n",
        "pdf.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0kIlbYbA-Qf"
      },
      "source": [
        "## 3.1 Cleaning data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPIwScZMA_WX"
      },
      "source": [
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
        "\n",
        "def clean_text_func(text):\n",
        "    text = text.lower() # lowercase text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
        "    return text\n",
        "\n",
        "def preprocessing_dataset(pdf_data, pdf):\n",
        "    pdf_data = pdf_data.apply(clean_text_func)\n",
        "    text_dataset = pdf[pdf_data.astype(bool)] \n",
        "    pdf = text_dataset.reset_index(drop = True)\n",
        "\n",
        "    text_data = pdf['text_reviews']\n",
        "    pdf_text_data = text_data.apply(clean_text_func)\n",
        "    pdf['text_reviews'] = pdf_text_data\n",
        "    return pdf\n",
        "\n",
        "pdf = preprocessing_dataset(pdf_data,pdf)\n",
        "pdf.head()\n",
        "pdf.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "-xKKC9tKFkmQ"
      },
      "source": [
        "#### 3.1 Viewing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h0iyk0RFkmR"
      },
      "source": [
        "sen_numbers = [x for x in set(pdf['sentence_id'])]\n",
        "def de_agument_text(pdf):\n",
        "  strings  = []\n",
        "  for i in sen_numbers:\n",
        "    anew = pdf.loc[pdf['sentence_id'] == i].sort_values('sentence_len', ascending=False)\n",
        "    seriess = anew[['text_reviews', 'class_id', 'sentence_len', 'class_']].iloc[0,0:4].values\n",
        "    strings.append(seriess)\n",
        "  return strings\n",
        "\n",
        "pdf_compressed = pd.DataFrame(de_agument_text(pdf), columns=['text_reviews', 'class_id', 'sentence_len', 'class_'])\n",
        "pdf_compressed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnZ_1_AgWPkT"
      },
      "source": [
        "print(pdf.shape)\n",
        "print(pdf_compressed.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfVMHKIh_uc9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "labels = {'negative':0,'somewhat negative':1,'neutral': 2,'somewhat positive':3,'positive':4}\n",
        "x = labels.keys()\n",
        "\n",
        "y_compressed = pdf_compressed.groupby('class_id').text_reviews.count()\n",
        "\n",
        "y_pdf = pdf.groupby('class_id').text_reviews.count()\n",
        "\n",
        "\n",
        "fig, [ax1, ax2]= plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
        "\n",
        "ax1.bar(x, y_compressed)\n",
        "ax1.set_xticklabels(labels = x, rotation =45)\n",
        "ax1.set_xlabel('Filtered Dataset: Class distribution', fontsize = 14)\n",
        "ax1.set_ylabel('Number of reviews in each class', fontsize = 14)\n",
        "\n",
        "ax2.bar(x, y_pdf)\n",
        "ax2.set_xticklabels(labels = x, rotation =45)\n",
        "ax2.set_xlabel('Original Dataset: Class distribution', fontsize = 14)\n",
        "ax2.set_ylabel('Number of reviews in each class', fontsize = 14)\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYEpd18vFkmS"
      },
      "source": [
        "from io import StringIO\n",
        "id_class_df = pdf[['class_id', 'class_']].drop_duplicates().sort_values('class_id')\n",
        "id_class = dict(id_class_df.values)\n",
        "class_id_dict = dict(id_class_df[['class_', 'class_id']].values)\n",
        "class_id_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryyKBJ4c8HFY"
      },
      "source": [
        "#Text Representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSWSNRfLri17"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FD8xDxZZu4E2"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(pdf['text_reviews'], pdf['class_id'],test_size=0.1, random_state = 0)\n",
        "X_train_reduced_text, X_test_reduced_text, y_train_reduced_text, y_test_reduced_text = train_test_split (pdf_compressed['text_reviews'], pdf_compressed['class_id'],test_size=0.1, random_state = 0)\n",
        "\n",
        "print (\"Numpy array of Original Dataset\")\n",
        "X_corpus = pdf.text_reviews.to_numpy()\n",
        "y_labels = pdf.class_id.to_numpy()\n",
        "X_corpus_smallds = pdf_compressed.text_reviews.to_numpy()\n",
        "y_labels_smallds = pdf_compressed.class_id.to_numpy()\n",
        "\n",
        "print (\"Training/Testing set from original Dataset\")\n",
        "X_train = X_train.to_numpy()\n",
        "X_test = X_test.to_numpy()\n",
        "y_train = y_train.to_numpy()\n",
        "y_test = y_test.to_numpy()\n",
        "\n",
        "print (\"Training/Testing set from redused Dataset\")\n",
        "X_train_reduced = X_train_reduced_text.to_numpy()\n",
        "X_test_reduced = X_test_reduced_text.to_numpy()\n",
        "y_train_reduced = y_train_reduced_text.to_numpy()\n",
        "y_test_reduced = y_test_reduced_text.to_numpy()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOGwC9RQz92f"
      },
      "source": [
        "#Verifying whether data is in the correct format\n",
        "\n",
        "print (\"Info of Original Dataset\")\n",
        "print(type(X_corpus_smallds))\n",
        "print(type(y_labels_smallds))\n",
        "print(X_corpus.shape)\n",
        "print(y_labels.shape)\n",
        "\n",
        "print(type(X_corpus))\n",
        "print(type(y_labels))\n",
        "print(X_corpus.shape)\n",
        "print(y_labels.shape)\n",
        "\n",
        "\n",
        "print (\"Training/Testing set from the original Dataset\")\n",
        "\n",
        "print(type(X_train))\n",
        "print(type(X_test))\n",
        "print(type(y_train))\n",
        "print(type(y_test))\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "print (\"Training/Testing set from the original Dataset\")\n",
        "\n",
        "print(type(X_train_reduced))\n",
        "print(type(X_test_reduced))\n",
        "print(type(y_train_reduced))\n",
        "print(type(y_test_reduced))\n",
        "\n",
        "print(X_train_reduced.shape)\n",
        "print(X_test_reduced.shape)\n",
        "print(y_train_reduced.shape)\n",
        "print(y_test_reduced.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7AnU8qZAS7A"
      },
      "source": [
        "# Linear SVC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iStp3itO7nl3"
      },
      "source": [
        "# Creating Pipeline\n",
        "# Model LinearSVC\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "pipeline = Pipeline([\n",
        "        ('vect', TfidfVectorizer(min_df=5, max_df=0.95, sublinear_tf=False, norm='l2', encoding='latin-1', stop_words='english')),\n",
        "        ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
        "        #('RandomF', RandomForestClassifier(n_estimators=500, max_depth=3, random_state=0)),\n",
        "        #('NB', MultinomialNB()),\n",
        "        #('clf', LinearSVC(C=1000)),\n",
        "        #('LR', LogisticRegression(max_iter = 1000, solver = 'lbfgs', random_state=0))\n",
        "    ])\n",
        "\n",
        "parameters = {\n",
        "        'vect__ngram_range': [(1, 1), (1,2)],\n",
        "    }\n",
        "\n",
        "#Fit the pipeline on the training set using grid search for the parameters\n",
        "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1)\n",
        "\n",
        "# train the training set\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "#make the prediction\n",
        "y_predicted = grid_search.predict(X_test)\n",
        "\n",
        "#Print the classification report\n",
        "print(metrics.classification_report(y_test, y_predicted,\n",
        "                                        target_names=pdf['class_'].unique()))\n",
        "\n",
        "# Print and plot the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test, y_predicted)\n",
        "fig, ax = plt.subplots(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt='d',\n",
        "            xticklabels=id_class_df.class_.values, yticklabels=id_class_df.class_.values)\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_k6fusIBYYy"
      },
      "source": [
        "# Multinomial NB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAxxlOhGBXHV"
      },
      "source": [
        "# Creating Pipeline\n",
        "# Model LinearSVC\n",
        "\n",
        "pipeline = Pipeline([\n",
        "        ('vect', TfidfVectorizer(min_df=5, max_df=0.95, sublinear_tf=True, norm='l2', encoding='latin-1', stop_words='english')),\n",
        "        ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
        "        #('RandomF', RandomForestClassifier(n_estimators=500, max_depth=3, random_state=0)),\n",
        "        #('NB', MultinomialNB()),\n",
        "        #('clf', LinearSVC(C=1000)),\n",
        "        #('LR', LogisticRegression(max_iter = 1000, solver = 'lbfgs', random_state=0))\n",
        "    ])\n",
        "\n",
        "parameters = {\n",
        "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
        "    }\n",
        "\n",
        "#Fit the pipeline on the training set using grid search for the parameters\n",
        "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1)\n",
        "\n",
        "# train the training set\n",
        "grid_search.fit(X_train_reduced, y_train_reduced)\n",
        "\n",
        "#make the prediction\n",
        "y_predicted_r = grid_search.predict(X_test_reduced)\n",
        "\n",
        "#Print the classification report\n",
        "print(metrics.classification_report(y_test_reduced, y_predicted_r,\n",
        "                                        target_names=pdf['class_'].unique()))\n",
        "\n",
        "# Print and plot the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test_reduced, y_predicted_r)\n",
        "fig, ax = plt.subplots(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt='d',\n",
        "            xticklabels=id_class_df.class_.values, yticklabels=id_class_df.class_.values)\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsru2OoyFkmS"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1,2), stop_words='english')\n",
        "\n",
        "features = tfidf.fit_transform(X_corpus_smallds).toarray()\n",
        "labels = y_labels_smallds\n",
        "features.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZK-Qk8NuIjw"
      },
      "source": [
        "from sklearn.feature_selection import chi2\n",
        "import numpy as np\n",
        "\n",
        "N = 3\n",
        "\n",
        "for class_id, class_ in sorted(id_class.items()):\n",
        "  features_chi2 = chi2(features, labels == class_id)\n",
        "  indices = np.argsort(features_chi2[0])\n",
        "  feature_names = np.array(tfidf.get_feature_names())[indices]\n",
        "\n",
        "  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
        "  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
        "  #trigrams = [v for v in feature_names if len(v.split(' ')) == 3]\n",
        "\n",
        "  print(\"# '{}':\".format(class_))\n",
        "  print ('=' * 50)\n",
        "  print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n",
        "  print ('-' * 30)\n",
        "  print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))\n",
        "  #print ('-' * 30)\n",
        "  #print(\"  . Most correlated trigrams:\\n. {}\".format('\\n. '.join(trigrams[-N:])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adXNBgcm77xF"
      },
      "source": [
        "### Verifying random text using multinomialNB Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R9TP0QsSZ1L"
      },
      "source": [
        "count_vect = CountVectorizer()\n",
        "X_train_counts = count_vect.fit_transform(X_train)\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Git050Sn9End"
      },
      "source": [
        "sample = pdf_compressed.sample().to_numpy()\n",
        "sample[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgqqBiUd_AGi"
      },
      "source": [
        "clf.predict(count_vect.transform([sample[0][0]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KmA7LIQ9xug"
      },
      "source": [
        "pdf_compressed[pdf_compressed['text_reviews'] == sample[0][0]]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}